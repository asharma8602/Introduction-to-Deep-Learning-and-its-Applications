{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "200155_Anshul_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFM645NE-D2"
      },
      "source": [
        "# Assignment 2\n",
        "In this assignment, we will go through Perceptron, Linear Classifiers, Loss Functions, Gradient Descent and Back Propagation.\n",
        "\n",
        "\n",
        "PS. this one is not from Stanford's course.\n",
        "\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "## Instructions\n",
        "* This notebook contain blocks of code, you are required to complete those blocks(where required)\n",
        "* You are required to copy this notebook (\"copy to drive\" above) and complete the code.(DO NOT CHANGE THE NAME OF THE FUNCTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QLtp15rqE-EU"
      },
      "source": [
        "# Part 1: Perceptron\n",
        "In this section, we will see how to implement a perceptron. Goal would be for you to delve into the mathematics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zao4e-DphaGA"
      },
      "source": [
        "## Intro\n",
        "What's a perceptron? It's an algorithm modelled on biological computational model to classify things into binary classes. It's a supervides learning algorithm, meaning that you need to provide labelled data containing features and the actual classifications. A perceptron would take these features as input and spit out a binary value (0 or 1). While training the model with training data, we try to minimise the error and learn the parameters involved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDTUoAd6ixm-"
      },
      "source": [
        "**How does it work?**\\\n",
        "A perceptron is modelled on a biological neuron. A neuron has input dendrites and the output is carried by axons. Similarly, a perceptron takes inputs called \"features\". After processing, a perceptron gives output. For computation, it has a \"weight\" vector which is multipled with feature vector. An activation function is added to introduce some non linearities and the output is given out.\\\n",
        "It can be represented as: $$  f=\\sum_{i=1}^{m} w_ix_i +b$$\n",
        "\n",
        "Let's implement this simple function to give an output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXezofBIgzId"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class perceptron():\n",
        "    def __init__(self,num_input_features=8):\n",
        "        self.weights = np.random.randn(num_input_features)\n",
        "        self.bias = np.random.random()\n",
        "\n",
        "    def activation(self,x):\n",
        "        '''\n",
        "            Implement heavside step activation function here (google ;))\n",
        "        '''\n",
        "        if x >= 0:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "\n",
        "    def forward(self,x: np.ndarray):\n",
        "        '''\n",
        "            you have random initialized weights and bias\n",
        "            you can access then using `self.weights` and `self.bias`\n",
        "            you should use activation function before returning\n",
        "        \n",
        "            x : input features\n",
        "            return : a binary value as the output of the perceptron \n",
        "        '''\n",
        "        w = self.weights  \n",
        "        b = self. bias \n",
        "        f = x.dot(w) + b\n",
        "        return self.activation(f)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKwDFAyocVo"
      },
      "source": [
        "np.random.seed(0)\n",
        "perc = perceptron(8)\n",
        "assert perc.forward(np.arange(8))==1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NWTTg1e9r7uM"
      },
      "source": [
        "# Part 2: Linear Classifier\n",
        "In this section, we will see how to implement a linear Classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDO4GcHr7uM"
      },
      "source": [
        "## Intro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HFvjH06r7uN"
      },
      "source": [
        "**How does it work?**\n",
        "\n",
        "Linear Classifier uses the following function: $$Y = WX+b$$ Where, $W$ is a 2d array of weights with shape (#classes, #features).\n",
        "\n",
        "\n",
        "\n",
        "Let's implement this classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A13CEkGr7uN"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearClassifier():\n",
        "    def __init__(self,num_input_features=32,num_classes=5):\n",
        "        self.weights = np.random.randn(num_classes,num_input_features)\n",
        "        self.bias = np.random.rand(num_classes,1)\n",
        "\n",
        "    def forward(self,x: np.ndarray):\n",
        "        '''\n",
        "            x: input features\n",
        "            you have random initialized weights and bias\n",
        "            you can access then using `self.weights` and `self.bias`\n",
        "            return an output vector of num_classes size\n",
        "        '''\n",
        "        return self.weights.dot(x) + self.bias\n",
        "        pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgzPxyTsr7uN",
        "outputId": "2263eff5-1a88-48a8-cf5b-981695266303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "lc = LinearClassifier()\n",
        "lc.forward(np.random.rand(32,1))\n",
        "# Should be close to:\n",
        "# array([[  7.07730669],\n",
        "    #    [-10.24067722],\n",
        "    #    [  0.75398702],\n",
        "    #    [  9.8019519 ],\n",
        "    #    [  2.36684038]])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  7.07730669],\n",
              "       [-10.24067722],\n",
              "       [  0.75398702],\n",
              "       [  9.8019519 ],\n",
              "       [  2.36684038]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZVgOVzJetuqo"
      },
      "source": [
        "# Part 3: Loss Functions, Gradient descent and Backpropagation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXryjpctuqy"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Loss Functions tells how \"off\" the output od our model is. Based upon the application, you can use several different loss functions. Formally, A loss function is a function $L:(z,y)\\in\\mathbb{R}\\times Y\\longmapsto L(z,y)\\in\\mathbb{R}$ that takes as inputs the predicted value $z$ corresponding to the real data value yy and outputs how different they are We'll implement L1 loss, L2 loss, Logistic loss, hinge loss and cross entropy loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGRb8BHotuqy"
      },
      "source": [
        "### **L1 loss**\n",
        "L1 loss is the linear loss function  $L = \\dfrac{1}{2}|y−z| $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxVh6IL2tuqz"
      },
      "source": [
        "import numpy as np\n",
        "def L1loss(z,y):\n",
        "    '''\n",
        "        y : True output.\n",
        "        z : Predicted output.\n",
        "        return : L\n",
        "    '''\n",
        "    return np.sum(0.5*abs(y-z))\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xy8ZS84cKtQ"
      },
      "source": [
        "### **L2 loss**\n",
        "L2 loss is the quadratic loss function or the least square error function  $L = \\dfrac{1}{2}(y−z)^2 $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JThp5P-KcKtS"
      },
      "source": [
        "import numpy as np\n",
        "def L2loss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    return np.sum(0.5*((y-z)**2))\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2JNLnWYcLSC"
      },
      "source": [
        "### **Hinge Loss**\n",
        "Hinge loss is: $ L = max( 0, 1 - yz ) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ1YM4J-cLSC"
      },
      "source": [
        "import numpy as np\n",
        "def hingeLoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = 0\n",
        "    for i in range(len(y)):\n",
        "      L += max(0,1-y[i]*z[i])\n",
        "    return L\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m15_MjradMNY"
      },
      "source": [
        "### **Cross Entropy Loss**\n",
        "Another very famous loss function is Cross Entropy loss: $ L = −[ylog(z)+(1−y)log(1−z)] $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snJLqhszdMNY"
      },
      "source": [
        "import numpy as np\n",
        "from math import *\n",
        "def CELoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = 0\n",
        "    for i in range(len(y)):\n",
        "      L -= (y[i]*log10(z[i]) + (1-y[i])*log10(1-z[i]))\n",
        "    return L\n",
        "    #pass"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsRPsfzxyEVL"
      },
      "source": [
        "### **0-1 Loss**\n",
        "Loss Function used by perceptron is: $ \\begin{cases} \n",
        "      0=z-y & z=y \\\\\n",
        "      1=\\dfrac{z-y}{z-y} & z\\neq y\n",
        "   \\end{cases} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sA7GxLHyEVM"
      },
      "source": [
        "import numpy as np\n",
        "def zeroOneLoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    sum = 0\n",
        "    if np.array_equal(z,y):\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "    \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWhbibHcgRR8"
      },
      "source": [
        "## Cost Function\n",
        "The cost function $J$ is commonly used to assess the performance of a model, and is defined with the loss function $L$ as follows:\n",
        "$$\\boxed{J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mL(h_\\theta(x^{(i)}), y^{(i)})}$$\n",
        "where $h_\\theta$ is the hypothesis function i.e. the function used to predict the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSbmhW4og97t"
      },
      "source": [
        "lossFunctions = {\n",
        "    \"l1\" : L1loss,\n",
        "    \"l2\" : L2loss,\n",
        "    \"hinge\" : hingeLoss,\n",
        "    \"cross-entropy\" : CELoss,\n",
        "    \"0-1\" : zeroOneLoss\n",
        "}\n",
        "\n",
        "def cost(Z : np.ndarray, Y : np.ndarray, loss : str):\n",
        "    '''\n",
        "        Z : a numpy array of predictions.\n",
        "        Y : a numpy array of true values.\n",
        "        return : A numpy array of costs calculated for each example.\n",
        "    '''\n",
        "    loss_func = lossFunctions[loss]\n",
        "    A=[]\n",
        "    for i in range(len()):  # should apply the loss function to each element of the arrays and return the array J\n",
        "      A.append(loss_func(Z[i],Y[i]))\n",
        "    return A"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upsN7A0zjGqx"
      },
      "source": [
        "## Gradient Descent and Back Propagation\n",
        "Gradient Descent is an algorithm that minimizes the loss function by calculating it's gradient. By noting $\\alpha\\in\\mathbb{R}$ the learning rate, the update rule for gradient descent is expressed with the learning rate $\\alpha$ and the cost function $J$ as follows:\n",
        "\n",
        "$$\\boxed{ W \\longleftarrow W -\\alpha\\nabla J( W )}$$\n",
        "​\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFCN-fYCqidi"
      },
      "source": [
        "But we need to find the partial derivative of Loss function wrt every parameter to know what is the slight change that we need to apply to our parameters. This becomes particularly hard if we have more than 1 layer in our algorithm. Here's where **Back Propagation** comes in. It's a way to find gradients wrt every parameter using the chain rule. Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. The derivative with respect to weight ww is computed using chain rule and is of the following form:\n",
        "\n",
        "$$\\boxed{\\frac{\\partial L(z,y)}{\\partial w}=\\frac{\\partial L(z,y)}{\\partial a}\\times\\frac{\\partial a}{\\partial z}\\times\\frac{\\partial z}{\\partial w}}$$\n",
        "​\n",
        " \n",
        "As a result, the weight is updated as follows:\n",
        "\n",
        "$$\\boxed{w\\longleftarrow w-\\alpha\\frac{\\partial L(z,y)}{\\partial w}}$$\n",
        "\n",
        "So, In a neural network, weights are updated as follows:\n",
        "\n",
        "* Step 1: Take a batch of training data.\n",
        "* Step 2: Perform forward propagation to obtain the corresponding loss.\n",
        "* Step 3: Backpropagate the loss to get the gradients.\n",
        "* Step 4: Use the gradients to update the weights of the network.\n",
        "​\n",
        "\n",
        "Bonus Problem\n",
        " \n",
        "Now, Assuming that you know Back Propagation (read a bit about it, if you don't), we'll now implement an image classification model on CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Problem**\n",
        "\n",
        "Now, Assuming that you know Back Propagation (read a bit about it, if you don't), we'll now implement an image classification model on CIFAR-10."
      ],
      "metadata": {
        "id": "sJoG5kkYopRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  \n",
        " \n",
        "# Display the version\n",
        "print(tf.__version__)    \n",
        " \n",
        "# other imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "_4-4RceVsor_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ab1ad6-a010-4f99-fe4c-c507bf3078a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyplk5PLEUsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65c7731-360f-4bec-822b-6dde2afebc5c"
      },
      "source": [
        "# Load in the data\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        " \n",
        "# Distribute it to train and test set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "# Reduce pixel values\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        " \n",
        "# flatten the label values\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQhkATYhEkkC"
      },
      "source": [
        "'''visualize data by plotting images'''\n",
        "# YOUR CODE HERE\n",
        "pass\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJgho2AEBFbx"
      },
      "source": [
        "\n",
        "# number of classes\n",
        "K = len(set(y_train))\n",
        "'''\n",
        " calculate total number of classes\n",
        " for output layer\n",
        "'''\n",
        "print(\"number of classes:\", K)\n",
        "''' \n",
        " Build the model using the functional API\n",
        " input layer\n",
        "'''\n",
        "```\n",
        "  YOUR CODE HERE\n",
        "```\n",
        " \n",
        "'''Hidden layer'''\n",
        "# YOUR CODE HERE\n",
        "pass\n",
        "# YOUR CODE HERE\n",
        " \n",
        "\"\"\"last hidden layer i.e.. output layer\"\"\"\n",
        "# YOUR CODE HERE\n",
        "pass\n",
        "# YOUR CODE HERE\n",
        " \n",
        " '''model description'''\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLc4Bay65TyA"
      },
      "source": [
        "# Compile\n",
        "...\n",
        "  YOUR CODE HERE\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit\n",
        "...\n",
        "  YOUR CODE HERE\n",
        "..."
      ],
      "metadata": {
        "id": "U0fGsDCRsQrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label mapping\n",
        " \n",
        "labels = '''airplane automobile bird cat deerdog frog horseship truck'''.split()\n",
        " \n",
        "# select the image from our test dataset\n",
        "image_number = 0\n",
        " \n",
        "# display the image\n",
        "plt.imshow(x_test[image_number])\n",
        " \n",
        "# load the image in an array\n",
        "n = np.array(x_test[image_number])\n",
        " \n",
        "# reshape it\n",
        "p = n.reshape(1, 32, 32, 3)\n",
        " \n",
        "# pass in the network for prediction and\n",
        "# save the predicted label\n",
        "predicted_label = labels[model.predict(p).argmax()]\n",
        " \n",
        "# load the original label\n",
        "original_label = labels[y_test[image_number]]\n",
        " \n",
        "# display the result\n",
        "print(\"Original label is {} and predicted label is {}\".format(\n",
        "    original_label, predicted_label))"
      ],
      "metadata": {
        "id": "RDq_RE6osSh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}