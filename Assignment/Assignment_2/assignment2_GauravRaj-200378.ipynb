{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvFM645NE-D2"
   },
   "source": [
    "# Assignment 2\n",
    "In this assignment, we will go through Perceptron, Linear Classifiers, Loss Functions, Gradient Descent and Back Propagation.\n",
    "\n",
    "\n",
    "PS. this one is not from Stanford's course.\n",
    "\n",
    "\n",
    "\n",
    "\\\n",
    "\n",
    "## Instructions\n",
    "* This notebook contain blocks of code, you are required to complete those blocks(where required)\n",
    "* You are required to copy this notebook (\"copy to drive\" above) and complete the code.(DO NOT CHANGE THE NAME OF THE FUNCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "QLtp15rqE-EU"
   },
   "source": [
    "# Part 1: Perceptron\n",
    "In this section, we will see how to implement a perceptron. Goal would be for you to delve into the mathematics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zao4e-DphaGA"
   },
   "source": [
    "## Intro\n",
    "What's a perceptron? It's an algorithm modelled on biological computational model to classify things into binary classes. It's a supervides learning algorithm, meaning that you need to provide labelled data containing features and the actual classifications. A perceptron would take these features as input and spit out a binary value (0 or 1). While training the model with training data, we try to minimise the error and learn the parameters involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDTUoAd6ixm-"
   },
   "source": [
    "**How does it work?**\\\n",
    "A perceptron is modelled on a biological neuron. A neuron has input dendrites and the output is carried by axons. Similarly, a perceptron takes inputs called \"features\". After processing, a perceptron gives output. For computation, it has a \"weight\" vector which is multipled with feature vector. An activation function is added to introduce some non linearities and the output is given out.\\\n",
    "It can be represented as: $$  f=\\sum_{i=1}^{m} w_ix_i +b$$\n",
    "\n",
    "Let's implement this simple function to give an output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iXezofBIgzId"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class perceptron():\n",
    "    def __init__(self,num_input_features=8):\n",
    "        self.weights = np.random.randn(num_input_features)\n",
    "        self.bias = np.random.random()\n",
    "\n",
    "    def activation(self,x):\n",
    "         x = np.transpose(x)\n",
    "         if ( (np.dot(x,self.weights) + self.bias))<0:\n",
    "            return 0 \n",
    "         else: \n",
    "            return 1\n",
    "          \n",
    "    \n",
    "        \n",
    "         \n",
    "\n",
    "    def forward(self,x: np.ndarray):\n",
    "        \n",
    "        if ( (np.dot(x,self.weights) + self.bias))<0:\n",
    "            return 0 \n",
    "        else: \n",
    "            return 1\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oSKwDFAyocVo"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "perc = perceptron(8)\n",
    "assert perc.forward(np.arange(8))==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "NWTTg1e9r7uM"
   },
   "source": [
    "# Part 2: Linear Classifier\n",
    "In this section, we will see how to implement a linear Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYDO4GcHr7uM"
   },
   "source": [
    "## Intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HFvjH06r7uN"
   },
   "source": [
    "**How does it work?**\n",
    "\n",
    "Linear Classifier uses the following function: $$Y = WX+b$$ Where, $W$ is a 2d array of weights with shape (#classes, #features).\n",
    "\n",
    "\n",
    "\n",
    "Let's implement this classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9A13CEkGr7uN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self,num_input_features=32,num_classes=5):\n",
    "        self.weights = np.random.randn(num_classes,num_input_features)\n",
    "        self.bias = np.random.rand(num_classes,1)\n",
    "\n",
    "    def forward(self,x: np.ndarray):\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return z\n",
    "        pass\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zgzPxyTsr7uN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.07730669],\n",
       "       [-10.24067722],\n",
       "       [  0.75398702],\n",
       "       [  9.8019519 ],\n",
       "       [  2.36684038]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "lc = LinearClassifier()\n",
    "lc.forward(np.random.rand(32,1))\n",
    "# Should be close to:\n",
    "# array([[  7.07730669],\n",
    "    #    [-10.24067722],\n",
    "    #    [  0.75398702],\n",
    "    #    [  9.8019519 ],\n",
    "    #    [  2.36684038]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ZVgOVzJetuqo"
   },
   "source": [
    "# Part 3: Loss Functions, Gradient descent and Backpropagation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pXryjpctuqy"
   },
   "source": [
    "## Intro\n",
    "\n",
    "Loss Functions tells how \"off\" the output od our model is. Based upon the application, you can use several different loss functions. Formally, A loss function is a function $L:(z,y)\\in\\mathbb{R}\\times Y\\longmapsto L(z,y)\\in\\mathbb{R}$ that takes as inputs the predicted value $z$ corresponding to the real data value yy and outputs how different they are We'll implement L1 loss, L2 loss, Logistic loss, hinge loss and cross entropy loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGRb8BHotuqy"
   },
   "source": [
    "### **L1 loss**\n",
    "L1 loss is the linear loss function  $L = \\dfrac{1}{2}|y−z| $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YxVh6IL2tuqz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def L1Loss(z,y):\n",
    "    l1 = 0.5*(abs(y-z))\n",
    "    return l1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xy8ZS84cKtQ"
   },
   "source": [
    "### **L2 loss**\n",
    "L2 loss is the quadratic loss function or the least square error function  $L = \\dfrac{1}{2}(y−z)^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JThp5P-KcKtS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def L2Loss(z,y):\n",
    "    l2 = 0.5*(y-z)**2\n",
    "    return l2\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2JNLnWYcLSC"
   },
   "source": [
    "### **Hinge Loss**\n",
    "Hinge loss is: $ L = max( 0, 1 - yz ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gQ1YM4J-cLSC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def hingeLoss(z,y):\n",
    "    l3 = max(0, 1 -y*z)\n",
    "    return l3\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m15_MjradMNY"
   },
   "source": [
    "### **Cross Entropy Loss**\n",
    "Another very famous loss function is Cross Entropy loss: $ L = −[ylog(z)+(1−y)log(1−z)] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "snJLqhszdMNY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "def CELoss(z,y):\n",
    "    l4 = -(y*mt.log(z) + (1-y)*mt.log(1-z))\n",
    "    return l4\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsRPsfzxyEVL"
   },
   "source": [
    "### **0-1 Loss**\n",
    "Loss Function used by perceptron is: $ \\begin{cases} \n",
    "      0=z-y & z=y \\\\\n",
    "      1=\\dfrac{z-y}{z-y} & z\\neq y\n",
    "   \\end{cases} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5sA7GxLHyEVM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def zeroOneLoss(z,y):\n",
    "    if z==y :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWhbibHcgRR8"
   },
   "source": [
    "## Cost Function\n",
    "The cost function $J$ is commonly used to assess the performance of a model, and is defined with the loss function $L$ as follows:\n",
    "$$\\boxed{J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mL(h_\\theta(x^{(i)}), y^{(i)})}$$\n",
    "where $h_\\theta$ is the hypothesis function i.e. the function used to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SSbmhW4og97t"
   },
   "outputs": [],
   "source": [
    "lossFunctions = {\n",
    "    \"l1\" : L1Loss,\n",
    "    \"l2\" : L2Loss,\n",
    "    \"hinge\" : hingeLoss,\n",
    "    \"cross-entropy\" : CELoss,\n",
    "    \"0-1\" : zeroOneLoss\n",
    "}\n",
    "\n",
    "def cost(Z : np.ndarray, Y : np.ndarray, loss : str):\n",
    "    '''\n",
    "        Z : a numpy array of predictions.\n",
    "        Y : a numpy array of true values.\n",
    "        return : A numpy array of costs calculated for each example.\n",
    "    '''\n",
    "    loss_func = lossFunctions[loss]\n",
    "    # YOUR CODE HERE\n",
    "    #use len(z) to calculate the value of m in the loss eqaution. then use the loss functions used above to calculate various losses. \n",
    "    l1 = L1loss(z,y)\n",
    "    l2 = L2loss(z,y)\n",
    "    l3= hingeLoss(z,y)\n",
    "    l4 = CELoss(z,y)\n",
    "    l5 = zeroOneLoss(z,y)\n",
    "    L = np.array([l1,l2l3,l4,l5])\n",
    "    return L\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upsN7A0zjGqx"
   },
   "source": [
    "## Gradient Descent and Back Propagation\n",
    "Gradient Descent is an algorithm that minimizes the loss function by calculating it's gradient. By noting $\\alpha\\in\\mathbb{R}$ the learning rate, the update rule for gradient descent is expressed with the learning rate $\\alpha$ and the cost function $J$ as follows:\n",
    "\n",
    "$$\\boxed{ W \\longleftarrow W -\\alpha\\nabla J( W )}$$\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFCN-fYCqidi"
   },
   "source": [
    "But we need to find the partial derivative of Loss function wrt every parameter to know what is the slight change that we need to apply to our parameters. This becomes particularly hard if we have more than 1 layer in our algorithm. Here's where **Back Propagation** comes in. It's a way to find gradients wrt every parameter using the chain rule. Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. The derivative with respect to weight ww is computed using chain rule and is of the following form:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L(z,y)}{\\partial w}=\\frac{\\partial L(z,y)}{\\partial a}\\times\\frac{\\partial a}{\\partial z}\\times\\frac{\\partial z}{\\partial w}}$$\n",
    "​\n",
    " \n",
    "As a result, the weight is updated as follows:\n",
    "\n",
    "$$\\boxed{w\\longleftarrow w-\\alpha\\frac{\\partial L(z,y)}{\\partial w}}$$\n",
    "\n",
    "So, In a neural network, weights are updated as follows:\n",
    "\n",
    "* Step 1: Take a batch of training data.\n",
    "* Step 2: Perform forward propagation to obtain the corresponding loss.\n",
    "* Step 3: Backpropagate the loss to get the gradients.\n",
    "* Step 4: Use the gradients to update the weights of the network.\n",
    "​\n",
    "\n",
    "Bonus Problem\n",
    " \n",
    "Now, Assuming that you know Back Propagation (read a bit about it, if you don't), we'll now implement an image classification model on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gaura\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=22694780774e106217e9d558b0930908577f4e3ee41a04af5ba42af01151546b\n",
      "  Stored in directory: c:\\users\\gaura\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 importlib-metadata-4.11.4 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_4-4RceVsor_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    " \n",
    "# Display the version\n",
    "print(tf.__version__)    \n",
    " \n",
    "# other imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJoG5kkYopRN"
   },
   "source": [
    "# **Bonus Problem**\n",
    "\n",
    "Now, Assuming that you know Back Propagation (read a bit about it, if you don't), we'll now implement an image classification model on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yyplk5PLEUsJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 59s 0us/step\n",
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    " \n",
    "# Distribute it to train and test set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# Reduce pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    " \n",
    "# flatten the label values\n",
    "y_train, y_test = y_train.flatten(), y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQhkATYhEkkC"
   },
   "outputs": [],
   "source": [
    "'''visualize data by plotting images'''\n",
    "# YOUR CODE HERE\n",
    "pass\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJgho2AEBFbx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# number of classes\n",
    "K = len(set(y_train))\n",
    "'''\n",
    " calculate total number of classes\n",
    " for output layer\n",
    "'''\n",
    "print(\"number of classes:\", K)\n",
    "''' \n",
    " Build the model using the functional API\n",
    " input layer\n",
    "'''\n",
    "```\n",
    "  YOUR CODE HERE\n",
    "```\n",
    " \n",
    "'''Hidden layer'''\n",
    "# YOUR CODE HERE\n",
    "pass\n",
    "# YOUR CODE HERE\n",
    " \n",
    "\"\"\"last hidden layer i.e.. output layer\"\"\"\n",
    "# YOUR CODE HERE\n",
    "pass\n",
    "# YOUR CODE HERE\n",
    " \n",
    " '''model description'''\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLc4Bay65TyA"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "...\n",
    "  YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0fGsDCRsQrn"
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "...\n",
    "  YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDq_RE6osSh8"
   },
   "outputs": [],
   "source": [
    "# label mapping\n",
    " \n",
    "labels = '''airplane automobile bird cat deerdog frog horseship truck'''.split()\n",
    " \n",
    "# select the image from our test dataset\n",
    "image_number = 0\n",
    " \n",
    "# display the image\n",
    "plt.imshow(x_test[image_number])\n",
    " \n",
    "# load the image in an array\n",
    "n = np.array(x_test[image_number])\n",
    " \n",
    "# reshape it\n",
    "p = n.reshape(1, 32, 32, 3)\n",
    " \n",
    "# pass in the network for prediction and\n",
    "# save the predicted label\n",
    "predicted_label = labels[model.predict(p).argmax()]\n",
    " \n",
    "# load the original label\n",
    "original_label = labels[y_test[image_number]]\n",
    " \n",
    "# display the result\n",
    "print(\"Original label is {} and predicted label is {}\".format(\n",
    "    original_label, predicted_label))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of DL_Stamatics_A2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
